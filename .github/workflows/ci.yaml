# .github/workflows/ci.yml
name: CI Pipeline with DagsHub

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

env:
  PYTHON_VERSION: '3.10'
  CACHE_VERSION: 'v3'

jobs:
  pipeline:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      
      - name: Cache pip packages
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('requirements.txt') }}-${{ env.CACHE_VERSION }}
      
      - name: Cache NLTK data
        uses: actions/cache@v4
        with:
          path: ~/nltk_data
          key: ${{ runner.os }}-nltk-${{ env.CACHE_VERSION }}
      
      - name: Cache DVC
        uses: actions/cache@v4
        with:
          path: .dvc/cache
          key: ${{ runner.os }}-dvc-${{ hashFiles('dvc.lock') }}-${{ env.CACHE_VERSION }}
      
      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: Setup NLTK
        run: |
          python -c "import nltk; nltk.download('stopwords', quiet=True); nltk.download('wordnet', quiet=True)"
      
      - name: Configure DVC for DagsHub
        env:
          DAGSHUB_TOKEN: ${{ secrets.DAGSHUB_TOKEN }}
        run: |
          echo "üîß Configuring DVC for DagsHub..."
          dvc remote add origin https://dagshub.com/BhautikVekariya21/ci.dvc -f || true
          dvc remote modify origin auth basic
          dvc remote modify origin user BhautikVekariya21
          dvc remote modify origin password $DAGSHUB_TOKEN
          dvc remote default origin
          echo "‚úÖ DVC configured"
      
      - name: Pull DVC data from DagsHub
        env:
          DAGSHUB_TOKEN: ${{ secrets.DAGSHUB_TOKEN }}
        run: |
          echo "üì• Pulling data from DagsHub..."
          dvc pull || echo "‚ö†Ô∏è No data to pull"
      
      - name: Run DVC pipeline with DagsHub MLflow
        env:
          CI: 'true'
          DAGSHUB_TOKEN: ${{ secrets.DAGSHUB_TOKEN }}
        run: |
          echo "üöÄ Starting pipeline with DagsHub integration..."
          dvc repro
          echo "‚úÖ Pipeline completed"
      
      - name: Display metrics
        run: |
          echo "üìä Model Performance:"
          cat evaluation/metrics.json | python -m json.tool || echo "No metrics"
      
      - name: Quick prediction tests
        run: |
          python << 'EOF'
          import pickle, pandas as pd, re, nltk
          from sklearn.feature_extraction.text import TfidfVectorizer
          from nltk.corpus import stopwords
          from nltk.stem import WordNetLemmatizer
          
          with open('models/model.pkl', 'rb') as f:
              model = pickle.load(f)
          
          train_df = pd.read_csv('data/processed/train_processed.csv')
          vectorizer = TfidfVectorizer(max_features=3000, ngram_range=(1, 2))
          vectorizer.fit(train_df['content'].fillna(''))
          
          class TextPreprocessor:
              def __init__(self):
                  self.lemmatizer = WordNetLemmatizer()
                  self.stop_words = set(stopwords.words('english'))
                  self.punct = r"""!"#$%&'()*+,-./:;<=>?@[\]^_`{|}~"""
              def clean_text(self, text):
                  if not isinstance(text, str) or not text.strip(): return ''
                  text = text.strip().lower()
                  text = re.sub(r'https?://\S+|www\.\S+', '', text)
                  text = text.translate(str.maketrans('', '', self.punct))
                  text = ''.join([i for i in text if not i.isdigit()])
                  text = ' '.join([w for w in text.split() if w not in self.stop_words])
                  text = ' '.join([self.lemmatizer.lemmatize(w) for w in text.split()])
                  return text.strip()
          
          preprocessor = TextPreprocessor()
          tests = [
              ('I love this!', 'happy'),
              ('So sad today.', 'sad'),
          ]
          
          print('\nüß™ Quick Tests:')
          for text, expected in tests:
              vec = vectorizer.transform([preprocessor.clean_text(text)])
              sentiment = 'happy' if model.predict(vec)[0] == 1 else 'sad'
              print(f'{"‚úÖ" if sentiment == expected else "‚ùå"} "{text}" -> {sentiment}')
          EOF
      
      - name: Run pytest
        run: pytest -v --tb=short || echo "‚ö†Ô∏è Some tests failed"
      
      - name: Push to DagsHub
        if: github.ref == 'refs/heads/main' && github.event_name == 'push'
        env:
          DAGSHUB_TOKEN: ${{ secrets.DAGSHUB_TOKEN }}
        run: |
          echo "üì§ Pushing to DagsHub..."
          dvc push || echo "‚ö†Ô∏è Push failed"
      
      - name: Upload artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: pipeline-results
          path: |
            evaluation/
            models/model.pkl
            mlruns/
          retention-days: 30
          if-no-files-found: warn
      
      - name: Create summary
        if: always()
        run: |
          echo "## üéØ Pipeline Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### üìä Metrics" >> $GITHUB_STEP_SUMMARY
          if [ -f evaluation/metrics.json ]; then
            echo "\`\`\`json" >> $GITHUB_STEP_SUMMARY
            cat evaluation/metrics.json >> $GITHUB_STEP_SUMMARY
            echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
          fi
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### üåê DagsHub Links" >> $GITHUB_STEP_SUMMARY
          echo "- üìä [MLflow Tracking](https://dagshub.com/BhautikVekariya21/ci.mlflow)" >> $GITHUB_STEP_SUMMARY
          echo "- üì¶ [DVC Storage](https://dagshub.com/BhautikVekariya21/ci.dvc)" >> $GITHUB_STEP_SUMMARY
          echo "- üåê [Main Repo](https://dagshub.com/BhautikVekariya21/ci)" >> $GITHUB_STEP_SUMMARY